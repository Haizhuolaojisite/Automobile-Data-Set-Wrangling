Model Evaluation
=========================================
1. In-sample evaluation tells us how well our model will fit the data used to train it
2. Problem:
    It doesn't tell us how well the trained model can be used to predict new data
3. Solution:
  split into in-sample data or training data (70%)
  and out-of-sample evaluation or test set (30%)

Build and train the model with a training set
Use testing set to assess the performance of a predictive model
When we have completed testing our model, we should use all the data to train the model to 
get the best performance.

Function train_test_split()
=============================
1. split data into random train and test subsets

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x_data,y_data,test_size = 0.3, random_state =0)

x_data: features or independent variables
y_data: dataset target: df['price']
x_train,y_train: parts of available data as training set 
x_test,y_test: parts of available data as testing set
test_size: percentage of the data for testing (here 30%)
random_state: number generator used for random sampling, a random seed for random data set splitting

Generalization Performance
=====================================
1. Generalization error is measure of how well our data does at predicting previously unseen data

2. The error we obtain using our testing data is an approximation of this error

Using a lot of data for training gives us an accurate means of determining how well our model will perform
in the real world.But the precision of the performance will be low.

The center of this bull's eye represents the correct generalization error. Let's say we take a random sample of the data using 90 percent of the data for training 
and 10 percent for testing. 
The first time we experiment, we get a good estimate of the training data.

If we experiment again training the model with a different combination of samples, we also get a good result. But, the results will be different 
relative to the first time we run the experiment. Repeating the experiment again with a different combination of training and testing samples, 
the results are relatively close to the generalization error, but distinct from each other. Repeating the process, 
we get good approximation of the generalization error, but the precision is poor 
i.e. all the results are extremely different from one another. 

If we use fewer data points to train the model and more to test the model, the accuracy of the generalization performance will be less, 
but the model will have good precision.All our error estimates are relatively close together, but they are further away from the true generalization performance.

To overcome this problem, we use cross-validation.
====================================================
1. most common out-of-sample evaluation metrics
2. more effective use of data (each observation is used for both training and testing)
3. The dataset is split into K equal groups, each group is referred to as a fold.
Some of the folds can be used as a training set which we use to train the model,
and the remaining parts are used as a test set, which we use to test the model.

For example, we can use 3 folds for training, then use 1 fold for testing.
This is repeated until each partition is used for both training and testing.
At the end, we use the avg results as the estimate of out-of-sample error. The evaluation metric depends on the model, e.g. r squared

Function cross_val_score()
=========================================================
from sklearn.model_selection import cross_val_score

scores = cross_val_score(lr,x_data,y_data,cv=3)

lr: initialze a linear regression model or object lr
x_data: predictive variable data
y_data: target variable data
cv: data set is split into 3 equal partitions

The function returns an array of scores, one of each partition that was chosen as the testing set.
We can avg the result together to estimate out of sample r^2:

np.mean(scores) or scores.mean()

First, we split the data into three folds. We use two folds for training, the remaining fold for testing. 
The model will produce an output. We will use the output to calculate a score. In the case of the r squared 
i.e. coefficient of determination, we will store that value in an array. We will repeat the process using two folds for training and one fold for testing. 
Save the score, then use a different combination for training and the remaining fold for testing. We store the final result. 

We want to know the actual predicted values supplied by our model before the r^2 values are calculated.
To do this, we use 
Function cross_val_predict()
=======================================================================
from sklearn.model_selection import cross_val_predict

yhat=cross_val_predict(lr2e,x_data,y_data,cv=3)

1. we split the data into 3 folds. We use 2 folds for training, the remaing fold for testing.
the model will produce an output, and we will store it in an array.

2. We will repeat the process using 2 folds for training and one for testing. The model produces an output again.

3. Finally, we use the last two folds for training and then we use the testing data.
This final testing fold produces an output. 

These predictions are stored in an array.


